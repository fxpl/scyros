# Skyscraper

## Setup

Rust must be installed in latest version. For the pipeline to work you will need GitHub personal access tokens. You can generate a token for your GH account by following the instructions 
[here](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token). 
 
All the tokens are stored in a csv file of your choice. The contents of this file needs to be one column with the header `token` containing a list of GitHub OAuth tokens, 
 one on each line. For example:

    token
    fa56454....
    hj73647.... 
    
Note that having multiple tokens from different accounts increases the throughput of the program. Having multiple tokens from the same account will not have any effect.

## Running the scraping framework

### Step 0: Setting up the configuration

The pipeline is written in Rust and Python. Before running Rust programs, please make sure that Rust is installed in latest version. Python 3.9.6 or later is required.

In order to run the Python scripts and notebooks, you need to have Python installed as well as the following dependencies that can be installed using pip:
- jupyter
- pandas
- nbstripout
- numpy
- matplotlib
- prettytable

All the dependencies can also be installed by running the following command:
```
pip install -r requirements.txt
```
where [requirements.txt](requirements.txt) contains the list of dependencies.

All the input and output file paths of the scripts are set in a settings file. As an example, [settings_example.py](settings_example.py) is provided. You need to copy this file and rename it to `settings.py`. Make sure to change the paths to the correct ones before running the scripts.

To compile Rust programs, run the following command in the root of the repository:
```
cargo build --release
```
All the binaries can then be found in the `target/release` folder.


### Step 1: ID scraper

The implementation of the ID scraper can be found in [random_id_sampling.rs](src/bin/random_id_sampling.rs). This program randomly samples ID of public repositories on github. To run the program, execute the following commands:
```
target/release/random-id-sampling -o {path_to_csv_file_with_ids} -t {path_to_csv_files_with_tokens}
```

If the program is interrupted and restarted afterwards, it will resume where it stopped. A message should be displayed every 1000 requests made to the API.

For more usage information, run:
```
target/release/random-id-sampling --help
```

### Step 2: Duplicate IDs removal

Executing the jupyter notebook [scripts/remove_duplicate_ids.ipynb](scripts/remove_duplicate_ids.ipynb) or the script [scripts/remove_duplicate_ids.py](scripts/remove_duplicate_ids.py) removes duplicate IDs from the file generated by the ID scraper.

### Step 3: Forks filtering

Executing the jupyter notebook [scripts/filter_forks.ipynb](scripts/filter_forks.ipynb) or the script [scripts/filter_forks.py](scripts/filter_forks.py) removes forks from the file generated by the ID scraper (where duplicates might have been removed by the previous script).


### Step 4: Metadata scraper

The implementation of the metadata scraper can be found in [metadata_scraper.rs](src/bin/metadata_scraper.rs). This program randomly chooses repositories from a file and fetches the metadata of those projects. A cache can be used to reduce the number of requests to GitHub API. To run the program, execute the following commands:
```
target/release/metadata-scraper -i {path_to_csv_file_with_ids} -t {path_to_csv_files_with_tokens} [-c {path_to_cache_file}]
```

The output of the program will be stored in `path_to_csv_file_with_ids.with_metadata`. If the program is interrupted and restarted afterwards, it will resume where it stopped. A message should be displayed every 1000 requests to the API.

For more usage information, run:
```
target/release/metadata-scraper --help
```

### Step 5: Discarding unininteresting projects

Executing the jupyter notebook [scripts/discard_uninteresting.ipynb](scripts/discard_uninteresting.ipynb) or the script [scripts/discard_uninteresting.py](scripts/discard_uninteresting.py) removes projects that are not interesting for the study. Before running the script, make sure your working directory is the root of this repository.

### Step 6: Language scraper

The implementation of the language scraper can be found in [language_scraper.rs](src/bin/language_scraper.rs). This program randomly chooses repositories from a file and fetch the list of languages of that project in addition top the hash of its commit. A cache can be used to reduce the number of requests to GitHub API. To run the program, execute the following commands:
```
target/release/language-scraper -i {path_to_csv_file_with_ids} -t {path_to_csv_files_with_tokens} [-c {path_to_cache_file}]
```

The output of the program will be stored in `path_to_csv_file_with_ids.with_lang`. If the program is interrupted and restarted afterwards, it will resume where it stopped. A message should be displayed every 1000 requests to the API.

For more usage information, run:
```
target/release/language-scraper --help
```

### Step 7: Select projects written in specific languages

Executing the jupyter notebook [scripts/filter_static_lang.ipynb](scripts/filter_static_lang.ipynb) or the script [scripts/filter_static_lang.py](scripts/filter_static_lang.py) keeps only the projects that have been written in one the user's selected programming languages. The list of languages can be modified in the settings file. Before running the script, make sure your working directory is the root of this repository.

### Step 8: Download the projects and keep only the ones related to floating point arithmetic

The implementation of the downloader can be found in [downloader.rs](src/bin/downloader.rs). This program randomly chooses repositories from a file and download their content at a given commit. It then only keeps files ending with one of the provided extensions. Files that do not contain one of the provided keywords are discarded. Finally symbolic links and empty folders are removed. To run the program, execute the following commands (assuming that your working directory is the root of this repository):
```
target/release/downloader -i {path_to_csv_file_with_ids} -t {path_to_csv_files_with_tokens} -d {path_to_target_folder} -k {path_to_extensions_and_keywords_file} [--skip]
```

The list of extensions and keywords must be stored in a json file with 2 entries:
- `keywords`: the list of keywords of which at least one must be present in each file. The keywords are case insensitive.
- `extensions`: the list of extensions to keep (without period). For each extension, you can provide a list of additional keywords. These keywords will be added to the global list of keywords when looking for files with that particular extension. If no keyword needs to be added, you can provide an empty list. 

For example, the following json file will keep only files written in Java, C or TypeScript and that mention floating point types in the file:

```json
{
    "extensions": {
        "java" : [], 
        "c" : [],
        "ts" : ["number"]
    },
    "keywords": ["double", "float"]
}
```
The program outputs two log files: one with the list of projects that have been downloaded (`path_to_csv_file_with_ids.project_log`) and one with the list of files that have been kept (`path_to_csv_file_with_ids.file_log`). Both log files feature statistics for each item such as the number of lines of code, the number of files per project, the number of keywords found in each file, etc. A skip flag can be provided to skip the download of the projects and only compute statistics on the already downloaded files.

For more usage information, run:
```
target/release/downloader --help
```

### Step 9: Files parsing and function extraction

The implementation of the parser can be found in [parser.rs](src/bin/parser.rs). This program parses the files downloaded in the previous step, extracts the functions containing of the provided keywiords in individual files and computes statistics for every function. Although only functions matching one of the keywords are kept, statistics are computed for all functions. To run the program, execute the following commands (assuming that your working directory is the root of this repository):
```
target/release/parser -i {path_to_csv_file_with_ids} -k {path_to_extensions_and_keywords_file} [-n {number_of_threads}]
```
The list of keywords must be stored in a json as described in the previous step. The program outputs a log file with the list of functions that have been extracted (`path_to_csv_file_with_ids.function_log`) and statistics such as the number of lines of code, words, keywords matched, loops, conditionals, statements nestings... If the number of threads is not provided, the program will not parallelize the parsing. For every file, the program will create a folder with the same name as the file and store the extracted functions in it.

## Pre-add hook

To ensure that the output of the notebooks are cleared and that the scripts match their corresponding notebooks, you can run the script [pre_add.sh](pre_add.sh) before running `git add`. Make sure to give the necessary permissions to run the script.


## License
This project is licensed under the Apache License 2.0 â€” see the [LICENSE](LICENSE) file for details.